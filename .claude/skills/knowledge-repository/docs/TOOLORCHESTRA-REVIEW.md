# ToolOrchestra Review: Lessons for A2I2

**Document Type**: Strategic Technical Review
**Source**: [NVIDIA ToolOrchestra](https://github.com/NVlabs/ToolOrchestra) | [Paper](https://arxiv.org/abs/2511.21689)
**Reviewed**: 2026-01-25
**Updated**: 2026-01-25 (Gemini Integration Analysis)
**Purpose**: Identify enhancement opportunities for A2I2 Enterprise AI Chief of Staff

---

## Executive Summary

ToolOrchestra introduces the **orchestration paradigm** - using a small, efficient model (8B parameters) to coordinate larger, more capable tools and models. This review identifies 7 key innovations and specific enhancement opportunities for A2I2.

**Core Insight**: Intelligence emerges not from a monolith but from a composite system where a lightweight "brain" strategically invokes specialized tools.

**Update (2026-01-25)**: The pending [Gemini Integration PR](https://github.com/therealchandlerbing/a2i2-beta-v1/pull/new/claude/add-gemini-integration-50QJu) already implements several key ToolOrchestra concepts, including an **Intelligent Model Router** and multi-model coordination. This review has been updated to reflect this alignment and identify remaining opportunities.

---

## 1. Key Innovations from ToolOrchestra

### 1.1 The Orchestration Paradigm

**What It Is**: Instead of relying on a single powerful model, ToolOrchestra uses a small orchestrator (8B) that:
- Decides WHEN to invoke tools
- Decides WHICH tools to invoke
- Decides in what ORDER to invoke them
- Combines results through multi-turn reasoning

**Performance Results**:
| Benchmark | Orchestrator-8B | GPT-5 | Cost Reduction |
|-----------|-----------------|-------|----------------|
| HLE | 37.1% | 35.1% | 2.5x cheaper |
| τ²-Bench | 80.2% | 77.7% | ~70% savings |
| FRAMES | 76.3% | 74.0% | ~70% savings |

**Key Takeaway**: A small coordinator can outperform larger monolithic models by strategically routing to specialized tools.

### 1.2 Multi-Objective Reward Design

ToolOrchestra's RL training balances three objectives:

```
R(τ) = {
  M_normalized · P    if outcome correct
  0                   otherwise
}

Where:
- M_normalized = normalized efficiency mask (cost + latency penalties as values 0-1)
- P = preference vector [p_tool1, p_tool2, ..., p_outcome, p_compute, p_latency]
```

**Three Reward Components**:
1. **Outcome Reward** (r_outcome): Binary - did trajectory solve the task?
2. **Efficiency Reward** (r_compute, r_latency): Penalize monetary cost and wall-clock time
3. **Preference Reward**: Align tool selection with user-specified preferences

**Innovation**: User preferences are encoded as a preference vector P, allowing real-time control over which tools the orchestrator favors.

### 1.3 Unified Tool Interface

All tools - from web search to GPT-5 - are exposed through a single JSON interface:

```json
{
  "name": "tool_name",
  "description": "What this tool does and when to use it",
  "parameters": {
    "type": "object",
    "properties": { ... }
  }
}
```

**LLMs as Tools**: Model descriptions are auto-generated by:
1. Sampling 10 training tasks
2. Recording model trajectories
3. Using another LLM to synthesize descriptions from performance

### 1.4 Bias Correction Through RL

Prompting alone creates systematic biases:
- **Self-enhancement bias**: Models favor their own variants (GPT-5 → GPT-5-mini)
- **Other-enhancement bias**: Weaker models over-defer to stronger ones (Qwen-8B → GPT-5 73%)

**Solution**: End-to-end RL with Group Relative Policy Optimization (GRPO) produces balanced tool selection learned from outcomes, not prompt engineering.

### 1.5 Diverse Tool Configurations

Training randomizes:
- Tool availability per instance
- Pricing schedules
- Tool subsets

**Result**: Robust generalization to unseen tools and cost configurations at inference time.

### 1.6 ToolScale Data Synthesis

Two-step pipeline for generating verifiable agentic training data:

1. **Environment Simulation**:
   - Generate domain-specific database schemas
   - Create tool APIs matching the domain
   - Populate with consistent, validated entries

2. **Task Generation**:
   - Propose diverse intents for the domain
   - Convert to specific tasks with ground-truth actions
   - Evolve tasks by adding complexity

**Quality Filtering**:
- Remove if golden function calls error
- Remove if LLMs can't solve in pass@8
- Remove if solvable without actions

### 1.7 Preference-Aware Evaluation

User preferences are explicit inputs that modify behavior:

```
Example:
PI = "I work at a company with confidential data. Prefer local/open-source tools."
P = [0, 1, 1, 1, 0, 0, 0, 0, 0]  # Favor local search, open models
```

---

## 2. Gemini Integration: ToolOrchestra Concepts Already Implemented

The pending Gemini Integration PR implements several ToolOrchestra concepts out of the box:

### 2.0.1 Intelligent Model Router (ALREADY BUILT)

The Gemini integration includes a **routing decision matrix** that mirrors ToolOrchestra's orchestration:

```python
# From gemini-config.json routing rules
{
  "condition": "task.type === 'image_generation'",
  "model": "gemini-3-pro-image-preview",
  "fallback": "gemini-2.5-flash-image"
},
{
  "condition": "task.contextSize > 200000",
  "model": "gemini-3-pro-preview",
  "fallback": "gemini-2.5-pro"
},
{
  "condition": "task.latency === 'fast'",
  "model": "gemini-2.5-flash",
  "fallback": "gemini-2.5-flash-lite"
}
```

**ToolOrchestra Alignment**: This is exactly the "decide WHICH tool to invoke" capability.

### 2.0.2 Multi-Model Coordination (ALREADY BUILT)

The Gemini integration supports routing between:
- **Claude** (nuanced conversation, empathy)
- **Gemini 3 Pro** (complex reasoning, 1M context)
- **Gemini 3 Flash** (speed, grounding)
- **Gemini 3 Pro Image** (visual generation)
- **Deep Research Agent** (autonomous research)
- **PersonaPlex** (primary voice, 170ms latency)
- **Gemini Live API** (voice + reasoning)

**ToolOrchestra Alignment**: This matches ToolOrchestra's "unified tool interface" with LLMs as tools.

### 2.0.3 Thinking Levels = Efficiency Control (ALREADY BUILT)

Gemini's thinking levels (`minimal`, `low`, `medium`, `high`) provide cost/latency control:

| Level | Use Case | Latency |
|-------|----------|---------|
| `minimal` | Ultra-fast, simple tasks | ~300ms |
| `low` | Chat, high-throughput | ~500ms |
| `medium` | General analysis | ~1-2s |
| `high` | Complex reasoning | ~3-5s |

**ToolOrchestra Alignment**: This is the efficiency reward optimization built into model selection.

### 2.0.4 Voice Provider Hierarchy (ALREADY BUILT)

```
Voice Request
    │
    ├─ Ultra-low latency (<200ms) → PersonaPlex (PRIMARY)
    ├─ Needs reasoning + voice → Gemini Live API
    └─ Text-to-speech → PersonaPlex or Gemini TTS
```

**ToolOrchestra Alignment**: Strategic tool selection based on task requirements.

### What's STILL Missing (ToolOrchestra Gaps)

Despite the Gemini integration, these ToolOrchestra concepts need implementation:

| ToolOrchestra Concept | Status in A2I2 |
|----------------------|----------------|
| **Outcome tracking** | Partial - episodic memory captures events, but not model success rates |
| **Cost tracking per request** | Missing - no per-request cost aggregation |
| **User preference vectors** | Missing - routing rules are static, not user-configurable |
| **Learning from outcomes** | Missing - routing doesn't improve based on past success |
| **Bias correction** | Missing - no detection of over-reliance on specific models |

---

## 3. Remaining Enhancement Opportunities for A2I2

The following enhancements build ON TOP of the Gemini integration:

### 3.1 Skill Orchestration Layer (ENHANCED)

**Current State**: Gemini integration routes MODELS, but not SKILLS. A2I2 skills are independent.

**Enhancement**: Add an Orchestration Layer that:
- Maintains a registry of available skills with capability descriptions
- Routes requests to optimal skill combinations
- Learns which skills work well together from episodic memory
- Balances skill cost (tokens, latency) against accuracy

**Implementation Sketch**:
```
┌─────────────────────────────────────────────────────────────────┐
│                    SKILL ORCHESTRATOR                           │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  Input: User Request + Context + Preferences            │   │
│  └────────────────────────┬────────────────────────────────┘   │
│                           ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  Skill Router (lightweight coordinator)                 │   │
│  │  - Query procedural memory for skill patterns           │   │
│  │  - Score skills by: relevance × confidence × efficiency │   │
│  │  - Generate orchestration plan                          │   │
│  └────────────────────────┬────────────────────────────────┘   │
│                           ▼                                     │
│  ┌──────────┬──────────┬──────────┬──────────┬──────────┐      │
│  │Knowledge │Research  │Calendar  │Email     │Code      │      │
│  │Repository│Skill     │Skill     │Skill     │Skill     │      │
│  └────┬─────┴────┬─────┴────┬─────┴────┬─────┴────┬─────┘      │
│       └──────────┴──────────┼──────────┴──────────┘             │
│                             ▼                                   │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │  Result Synthesizer                                     │   │
│  │  - Combine skill outputs                                │   │
│  │  - Capture orchestration pattern (LEARN)                │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 Procedural Memory for Tool/Model Patterns (NEW)

**Current State**: Procedural memory captures workflows and preferences.

**Enhancement**: Extend to capture TOOL USAGE PATTERNS with outcomes:

```python
# New procedural memory type: tool_pattern
{
    "type": "tool_pattern",
    "pattern_id": "uuid",
    "task_context": "summarize financial report",
    "tools_used": ["web_search", "code_interpreter", "claude_analysis"],
    "tool_sequence": [
        {"tool": "web_search", "params": {...}, "success": true, "latency_ms": 450},
        {"tool": "code_interpreter", "params": {...}, "success": true, "latency_ms": 1200}
    ],
    "outcome": "success",
    "total_cost": 0.0023,
    "total_latency_ms": 2100,
    "confidence": 0.85,
    "usage_count": 7,
    "last_used": "2026-01-25T10:30:00Z"
}
```

**Benefits**:
- Cross-session learning of effective tool combinations
- Cost/latency optimization over time
- Personalized tool routing based on user history

### 3.3 User Preference Vectors (NEW)

**Current State**: Preferences are stored as procedural memory items (text-based).

**Enhancement**: Add structured preference vectors that modify orchestration behavior:

```python
class UserPreferenceVector:
    """Numerical preferences that modify tool/skill selection."""

    # Tool preferences (0.0 = avoid, 1.0 = strongly prefer)
    tool_preferences: Dict[str, float] = {
        "web_search": 0.5,      # Neutral on web search
        "local_search": 0.9,   # Strong preference for local
        "gpt_5": 0.3,          # Avoid expensive models
        "claude_local": 0.8,   # Prefer local Claude
    }

    # Objective weights
    accuracy_weight: float = 0.6    # How much to prioritize accuracy
    cost_weight: float = 0.3        # How much to minimize cost
    latency_weight: float = 0.1     # How much to minimize latency

    # Context-specific overrides
    context_overrides: Dict[str, Dict] = {
        "confidential_data": {"web_search": 0.0, "local_search": 1.0},
        "time_critical": {"latency_weight": 0.7, "cost_weight": 0.1}
    }
```

**Integration with LEARN**:
```python
def update_preference_vector(self, feedback: str, context: dict):
    """Update preferences based on user feedback."""
    # "I prefer using local tools for sensitive data"
    # → Increase local_search preference in confidential context
```

### 3.4 Efficiency Tracking in Autonomy Audit

**Current State**: `arcus_autonomy_audit` tracks actions and boundaries.

**Enhancement**: Add efficiency metrics to every autonomous action:

```sql
ALTER TABLE arcus_autonomy_audit ADD COLUMN (
    tokens_used INTEGER,
    estimated_cost DECIMAL(10, 6),
    latency_ms INTEGER,
    tools_invoked JSONB,  -- Array of tool names and costs
    efficiency_score DECIMAL(3, 2)  -- Computed: accuracy / (cost + latency_normalized)
);
```

**Usage**:
- REFLECT operations can analyze efficiency trends
- Identify high-cost patterns that could be optimized
- Provide cost visibility to users

### 3.5 Multi-Model Coordination (ALREADY IN GEMINI PR)

**Current State**: A2I2 assumes single Claude instance.

**Enhancement**: Architecture for coordinating multiple models as tools:

```yaml
# New config: model_tools.yaml
model_tools:
  - name: "claude_haiku"
    description: "Fast, cheap model for simple tasks, classification, extraction"
    cost_per_1k_tokens: 0.00025
    avg_latency_ms: 200
    capabilities: ["classification", "extraction", "simple_qa"]

  - name: "claude_sonnet"
    description: "Balanced model for most tasks"
    cost_per_1k_tokens: 0.003
    avg_latency_ms: 800
    capabilities: ["reasoning", "coding", "analysis", "creative"]

  - name: "claude_opus"
    description: "Most capable model for complex reasoning"
    cost_per_1k_tokens: 0.015
    avg_latency_ms: 2000
    capabilities: ["complex_reasoning", "research", "synthesis"]

  - name: "code_model"
    description: "Specialized coding model"
    endpoint: "local:codestral-22b"
    capabilities: ["code_generation", "code_review", "debugging"]
```

**Orchestration Logic**:
```python
def route_to_model(task: Task, preferences: UserPreferenceVector) -> str:
    """Select optimal model for task based on requirements and preferences."""
    required_capabilities = analyze_task_requirements(task)

    candidates = [m for m in MODEL_TOOLS if
                  set(required_capabilities) <= set(m.capabilities)]

    # Score by: capability_match × preference × (1 / (cost × cost_weight + latency × latency_weight))
    scored = [(m, compute_score(m, preferences)) for m in candidates]
    return max(scored, key=lambda x: x[1])[0]
```

### 3.6 Synthetic Data Generation for Skills

**Current State**: Skills are defined manually in SKILL.md.

**Enhancement**: Adopt ToolScale's approach to generate training/evaluation data:

1. **Domain Definition**: Define skill domains (client management, research, scheduling)
2. **Schema Generation**: Create data schemas for each domain
3. **Task Synthesis**: Generate diverse tasks with ground-truth skill sequences
4. **Complexity Evolution**: Automatically add constraints to increase difficulty

**Example Output**:
```json
{
  "domain": "client_management",
  "task": "Find all interactions with TechCorp in the last quarter and summarize the relationship status",
  "golden_skill_sequence": [
    {"skill": "recall", "params": {"query": "TechCorp", "memory_types": ["episodic"]}},
    {"skill": "get_entity_relationships", "params": {"entity": "TechCorp", "type": "organization"}},
    {"skill": "reflect", "params": {"focus_areas": ["TechCorp relationship"]}}
  ],
  "expected_output_contains": ["meeting count", "decision outcomes", "relationship health"]
}
```

### 3.7 Normalized Reward Signals for Skill Learning

**Current State**: Confidence scores (0.0-1.0) track memory reliability.

**Enhancement**: Add reward signals that can be used for future skill optimization:

```python
def compute_skill_reward(
    trajectory: List[SkillExecution],
    outcome: Outcome,
    user_preference: UserPreferenceVector
) -> float:
    """Compute normalized reward for a skill execution trajectory."""

    if not outcome.success:
        return 0.0

    # Normalize each component within recent history
    accuracy_score = outcome.accuracy  # 0-1
    cost_normalized = 1.0 - (trajectory.total_cost / max_cost_in_batch)
    latency_normalized = 1.0 - (trajectory.total_latency / max_latency_in_batch)

    # Apply preference weights
    reward = (
        accuracy_score * user_preference.accuracy_weight +
        cost_normalized * user_preference.cost_weight +
        latency_normalized * user_preference.latency_weight
    )

    # Add tool preference bonuses
    for skill in trajectory.skills:
        reward += user_preference.tool_preferences.get(skill.name, 0.5) * 0.1

    return reward
```

### 3.8 Context Window Optimization

**Current State**: Max 2000 tokens for context injection.

**Enhancement**: Dynamic context budgeting based on task complexity:

```python
def compute_context_budget(task: Task, available_context: List[Memory]) -> int:
    """Dynamically allocate context tokens based on task needs."""

    base_budget = 2000

    # Increase for complex tasks
    if task.complexity == "high":
        base_budget *= 1.5

    # Decrease if using expensive downstream model
    if task.target_model == "opus":
        base_budget *= 0.7  # Save tokens for expensive inference

    # Prioritize high-confidence, recent, relevant context
    context_value = sum(
        m.confidence * m.recency_score * m.relevance_score
        for m in available_context[:20]
    )

    if context_value > threshold:
        base_budget *= 1.2  # Worth including more context

    return min(base_budget, 4000)  # Hard cap
```

### 3.9 Skill Generalization Testing

**Current State**: Skills are tested ad-hoc.

**Enhancement**: Systematic generalization testing inspired by ToolOrchestra:

```python
# Test skill with unseen configurations
generalization_tests = [
    {
        "name": "unseen_entities",
        "setup": "Use entities not in training",
        "expect": "Skill handles gracefully"
    },
    {
        "name": "resource_constraints",
        "setup": "Limit available tools/models",
        "expect": "Skill adapts to available resources"
    },
    {
        "name": "preference_shifts",
        "setup": "Apply different preference vectors",
        "expect": "Skill respects preferences while maintaining accuracy"
    }
]
```

### 3.10 Autonomy Trust as Reward Signal

**Current State**: Autonomy levels (0-4) gate allowed actions.

**Enhancement**: Use autonomy progression as a long-term reward signal:

```python
def update_trust_metrics(action: AutonomousAction, outcome: Outcome):
    """Update trust metrics based on action outcomes."""

    # Track success rate by action type
    metrics.update_success_rate(action.type, outcome.success)

    # Compute trust delta
    if outcome.success and not outcome.required_correction:
        trust_delta = +0.01  # Small positive for clean success
    elif outcome.success and outcome.required_correction:
        trust_delta = -0.005  # Minor negative for corrected success
    else:
        trust_delta = -0.02  # Negative for failure

    # Update autonomy level if threshold crossed
    if metrics.trust_score > LEVEL_THRESHOLDS[current_level + 1]:
        propose_autonomy_upgrade(current_level + 1)
```

### 3.11 Voice-Optimized (ALREADY IN GEMINI PR) Orchestration

**Current State**: VNKG concept defined but not implemented.

**Enhancement**: Optimize orchestration for voice interactions:

```python
class VoiceOrchestrationConfig:
    """Config for voice-optimized skill orchestration."""

    max_response_time_ms: int = 500  # Voice needs fast responses
    prefer_streaming: bool = True     # Stream partial results

    # Favor fast skills for voice
    voice_skill_weights: Dict[str, float] = {
        "recall": 1.0,           # Fast lookup
        "quick_answer": 1.0,     # Direct response
        "reflect": 0.3,          # Too slow for voice
        "deep_research": 0.2,    # Background only
    }

    # Interrupt handling
    allow_interrupt: bool = True
    interrupt_saves_context: bool = True  # Save partial work
```

### 3.12 Federated Skill Learning

**Current State**: FOI concept defined for privacy-preserving learning.

**Enhancement**: Apply to skill patterns:

```python
class FederatedSkillLearning:
    """Learn skill patterns across organizations without sharing raw data."""

    def compute_local_gradient(self, skill_outcomes: List[Outcome]) -> Gradient:
        """Compute local gradient from skill outcomes."""
        # Aggregate patterns locally
        patterns = extract_skill_patterns(skill_outcomes)
        # Compute gradient (what worked, what didn't)
        return compute_pattern_gradient(patterns)

    def aggregate_gradients(self, gradients: List[Gradient]) -> GlobalUpdate:
        """Securely aggregate gradients from multiple orgs."""
        # Differential privacy applied here
        noised_sum = sum(g + noise for g in gradients)
        return GlobalUpdate(noised_sum / len(gradients))

    def apply_global_update(self, update: GlobalUpdate):
        """Apply aggregated learning to local skill router."""
        self.skill_router.update_weights(update)
```

---

## 4. Updated Implementation Priorities

### Phase 1: Foundation (Immediate)

| Enhancement | Effort | Impact | Files to Modify |
|-------------|--------|--------|-----------------|
| Efficiency tracking in audit | Low | Medium | `supabase-schema.sql`, `knowledge_operations.py` |
| Tool pattern procedural memory | Medium | High | `types.ts`, `knowledge_operations.py`, `SKILL.md` |
| Preference vectors (basic) | Medium | High | New: `preference_vectors.py`, modify `SKILL.md` |

### Phase 2: Orchestration (Near-term)

| Enhancement | Effort | Impact | Files to Modify |
|-------------|--------|--------|-----------------|
| Skill Orchestration Layer | High | Very High | New: `skill_orchestrator.py`, `orchestrator_config.yaml` |
| Multi-model coordination | Medium | High | New: `model_tools.yaml`, modify routing logic |
| Dynamic context budgeting | Medium | Medium | `knowledge_operations.py` |

### Phase 3: Learning (Medium-term)

| Enhancement | Effort | Impact | Files to Modify |
|-------------|--------|--------|-----------------|
| Normalized reward signals | Medium | High | `knowledge_operations.py`, new metrics |
| Synthetic data generation | High | High | New: `data_synthesis/` directory |
| Autonomy trust as reward | Medium | Medium | `arcus_autonomy_audit` triggers |

### Phase 4: Advanced (Long-term)

| Enhancement | Effort | Impact | Files to Modify |
|-------------|--------|--------|-----------------|
| Voice-optimized orchestration | High | High | New: `voice_orchestrator.py` |
| Federated skill learning | Very High | Very High | New subsystem |
| Generalization testing | Medium | Medium | New: `tests/generalization/` |

---

## 5. Key Takeaways

### What ToolOrchestra Gets Right

1. **Small coordinators beat large monoliths** - Strategic routing > brute force capability
2. **Multi-objective optimization is essential** - Accuracy alone isn't enough; efficiency and preferences matter
3. **Diverse training configurations** - Robustness requires exposure to varied tool/cost scenarios
4. **Unified interfaces** - Treat all tools (including LLMs) the same way
5. **Explicit preference control** - Users should influence tool selection at inference time

### How A2I2 Can Differentiate

1. **Persistent learning** - A2I2's memory architecture enables cross-session skill improvement
2. **Trust progression** - Autonomy levels provide natural reward signals
3. **Relationship context** - Knowledge graph enables smarter routing based on entity relationships
4. **Voice-native design** - PersonaPlex integration enables real-time orchestration
5. **Enterprise focus** - Privacy, audit trails, and organizational memory are first-class concerns

### Architectural Alignment (Updated with Gemini Integration)

```
ToolOrchestra Concept          →    A2I2 Equivalent              →    Status
───────────────────────────────────────────────────────────────────────────────
Orchestrator Model             →    Intelligent Model Router     →    BUILT (Gemini PR)
                               →    + Skill Orchestration Layer  →    PLANNED
Tool Descriptions              →    SKILL.md + gemini-config     →    BUILT (Gemini PR)
Multi-Model Tools              →    Claude + Gemini + PersonaPlex→    BUILT (Gemini PR)
Thinking Levels                →    thinkingConfig levels        →    BUILT (Gemini PR)
Voice Routing                  →    PersonaPlex/Gemini Live      →    BUILT (Gemini PR)
Outcome Reward                 →    Episodic outcomes            →    PARTIAL (needs model tracking)
Efficiency Reward              →    Autonomy audit metrics       →    PLANNED (needs per-request cost)
Preference Reward              →    User Preference Vectors      →    PLANNED (needs implementation)
ToolScale Dataset              →    Synthetic skill training     →    PLANNED
GRPO Training                  →    Procedural pattern learning  →    PLANNED
```

**Key Insight**: The Gemini Integration PR brings A2I2 ~60% of the way to ToolOrchestra's architecture. The remaining 40% is about **learning from outcomes** rather than static rules.

---

## 6. Conclusion

ToolOrchestra demonstrates that **intelligent coordination of specialized tools beats monolithic capability**.

**Good News**: The pending Gemini Integration PR already implements the foundational orchestration architecture:
- Multi-model routing (Claude, Gemini 3 Pro/Flash, Deep Research)
- Efficiency controls (thinking levels, fallback strategies)
- Voice provider hierarchy (PersonaPlex as primary)
- Capability-based routing rules

**Remaining Work** (to achieve full ToolOrchestra parity):
1. **Outcome Tracking**: Track success rates per model/skill, not just events
2. **Cost Visibility**: Add per-request cost aggregation to autonomy audit
3. **User Preference Vectors**: Make routing rules configurable, not static
4. **Learning from Outcomes**: Improve routing based on past success patterns
5. **Skill Orchestration**: Extend model routing to skill coordination

The combination of A2I2's persistent memory architecture with ToolOrchestra's orchestration principles creates a powerful hybrid: an AI Chief of Staff that not only remembers and learns, but also efficiently coordinates its capabilities to serve users with minimal cost and maximum alignment to preferences.

**The Gemini Integration is the foundation. The next step is making it adaptive.**

---

## References

- [ToolOrchestra Paper](https://arxiv.org/abs/2511.21689)
- [GitHub Repository](https://github.com/NVlabs/ToolOrchestra)
- [ToolScale Dataset](https://huggingface.co/datasets/nvidia/ToolScale)
- [NVIDIA Technical Blog](https://developer.nvidia.com/blog/train-small-orchestration-agents-to-solve-big-problems/)
- [Orchestrator-8B Model](https://huggingface.co/nvidia/Orchestrator-8B)
